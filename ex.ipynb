{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.10.11)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f3fef56d5894c84b3f12aa9e03d20dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "collecting trajectories:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrandom_walk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomWalkEnv\n\u001b[1;32m      4\u001b[0m env \u001b[38;5;241m=\u001b[39m RandomWalkEnv(num_nodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, weight_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, reach_the_goal_reward\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, max_episode_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mRandomWalkDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trajectories\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mdataset_size())\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mget_item(\u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/Documents/MIT/6.8200/project/data/random_walk_dataset.py:19\u001b[0m, in \u001b[0;36mRandomWalkDataset.__init__\u001b[0;34m(self, env, n_trajectories, reward_scale)\u001b[0m\n\u001b[1;32m     17\u001b[0m policy \u001b[38;5;241m=\u001b[39m RandomPolicy(env)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrajectories \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrajectories \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_trajectories\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trajectories\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trajectories\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# todo other policies maybe?\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# todo perhaps this should be based on the best path in the graph?\u001b[39;00m\n\u001b[1;32m     23\u001b[0m all_state_features \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Documents/MIT/6.8200/project/data/trajectory.py:136\u001b[0m, in \u001b[0;36mTrajectoryDataset.collect_trajectories\u001b[0;34m(self, env, policy, n_trajectories, step_limit)\u001b[0m\n\u001b[1;32m    134\u001b[0m res \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(n_trajectories), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollecting trajectories\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 136\u001b[0m     traj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_trajectory\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_limit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m     res\u001b[38;5;241m.\u001b[39mappend(traj)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/Documents/MIT/6.8200/project/data/trajectory.py:119\u001b[0m, in \u001b[0;36mTrajectoryDataset.collect_trajectory\u001b[0;34m(self, env, policy, step_limit)\u001b[0m\n\u001b[1;32m    117\u001b[0m observations, actions, rewards, dones \u001b[38;5;241m=\u001b[39m [], [], [], []\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(step_limit):\n\u001b[0;32m--> 119\u001b[0m     action, info \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     action \u001b[38;5;241m=\u001b[39m action\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    121\u001b[0m     observations\u001b[38;5;241m.\u001b[39mappend(observation)\n",
      "File \u001b[0;32m~/Documents/MIT/6.8200/project/algorithms/random_policy.py:23\u001b[0m, in \u001b[0;36mRandomPolicy.predict\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs):\n\u001b[0;32m---> 23\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mac_space\u001b[38;5;241m.\u001b[39msample() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch)]), {}\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "from data.random_walk_dataset import RandomWalkDataset\n",
    "from envs.random_walk import RandomWalkEnv\n",
    "\n",
    "env = RandomWalkEnv(num_nodes=5, weight_max=10, reach_the_goal_reward=100, max_episode_length=1024)\n",
    "dataset = RandomWalkDataset(env= env, n_trajectories=1000)\n",
    "print(dataset.dataset_size())\n",
    "print(dataset.get_item(0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
