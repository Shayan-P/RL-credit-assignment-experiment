{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-08T02:32:16.956825Z",
     "start_time": "2024-05-08T02:32:16.681644Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from envs.random_walk import RandomWalkEnv\n",
    "from algorithms.sequence_models.old_decision_transformer.decision_transformer.decision_transformer import DecisionTransformer\n",
    "from algorithms.sequence_models.old_decision_transformer.decision_transformer.trainer import DecisionTransformerTrainer\n",
    "from data.random_walk_dataset import RandomWalkDataset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T02:32:18.895818Z",
     "start_time": "2024-05-08T02:32:17.036957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = RandomWalkDataset()\n",
    "dataset"
   ],
   "id": "69cc35d77e943625",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Info:\n",
      "episode_max_length: 8\n",
      "reward_scale: 9\n",
      "return min=-17, max=9 mean=-3.514132262789562\n",
      "state_mean: [0.4028373  0.2299273  0.11319373 0.18999891 0.03674117 0.02730158]\n",
      "state_std: [0.49046856 0.42078586 0.31682946 0.39230005 0.18812564 0.16296074]\n",
      "gamma: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<data.random_walk_dataset.RandomWalkDataset at 0x7f9da5d4cd90>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "61ac1d1fbc9d20a6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T02:32:19.868194Z",
     "start_time": "2024-05-08T02:32:19.818404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = DecisionTransformer(\n",
    "        hidden_size=64,\n",
    "        dataset=dataset,\n",
    "        block_size=64,  # todo experiment with block_size. we might need to have block_size >= SPLIT_SEQUENCE_LENGTH but I'm not sure about this\n",
    "        max_length=None,\n",
    "        action_tanh=True,\n",
    "        gpt_config={}\n",
    ").to(device)\n",
    "model"
   ],
   "id": "3bc1d05e9f2c64b1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.09M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTransformer(\n",
       "  (transformer): GPT(\n",
       "    (transformer): ModuleDict(\n",
       "      (wte): Embedding(1, 48)\n",
       "      (wpe): Embedding(64, 48)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-2): 3 x Block(\n",
       "          (ln_1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): CausalSelfAttention(\n",
       "            (c_attn): Linear(in_features=48, out_features=144, bias=True)\n",
       "            (c_proj): Linear(in_features=48, out_features=48, bias=True)\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): ModuleDict(\n",
       "            (c_fc): Linear(in_features=48, out_features=192, bias=True)\n",
       "            (c_proj): Linear(in_features=192, out_features=48, bias=True)\n",
       "            (act): NewGELU()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=48, out_features=1, bias=False)\n",
       "  )\n",
       "  (embed_timestep): Embedding(8, 64)\n",
       "  (embed_return): Linear(in_features=1, out_features=64, bias=True)\n",
       "  (embed_state): Linear(in_features=6, out_features=64, bias=True)\n",
       "  (embed_action): Linear(in_features=6, out_features=64, bias=True)\n",
       "  (embed_ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (predict_state): Linear(in_features=64, out_features=6, bias=True)\n",
       "  (predict_action): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=6, bias=True)\n",
       "    (1): Tanh()\n",
       "  )\n",
       "  (predict_return): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T02:32:22.921734Z",
     "start_time": "2024-05-08T02:32:22.889868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# obs, _ = env.reset()\n",
    "# rtg = 10\n",
    "# \n",
    "# \n",
    "# states = torch.from_numpy(obs).unsqueeze(0)\n",
    "# actions = torch.zeros((0, model.act_dim), device=device, dtype=torch.float32)\n",
    "# rewards = torch.zeros(0, device=device, dtype=torch.float32)\n",
    "# rtgs = []\n",
    "# timesteps = []\n",
    "# \n",
    "# for i in range(10):\n",
    "#     action = env.action_space.sample()\n",
    "#     obs, reward, truncated, terminated, _ = env.step(action)\n",
    "#     done = truncated or terminated\n",
    "#     print(dataset.transform_to_feature_space(obs, action, reward, rtg, done))\n",
    "#     model(obs, action, reward, done)\n",
    "#     rtg -= reward\n",
    "#     if done:\n",
    "#         break\n"
   ],
   "id": "534321f928855af1",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T02:32:23.176165Z",
     "start_time": "2024-05-08T02:32:23.172995Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "7255ba4b038d114e",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T02:32:23.338214Z",
     "start_time": "2024-05-08T02:32:23.334585Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "8a3247c56894fc62",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T02:32:23.550168Z",
     "start_time": "2024-05-08T02:32:23.519416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--env', type=str, default='hopper')\n",
    "# parser.add_argument('--dataset', type=str, default='medium')  # medium, medium-replay, medium-expert, expert\n",
    "# parser.add_argument('--mode', type=str, default='normal')  # normal for standard setting, delayed for sparse\n",
    "# parser.add_argument('--K', type=int, default=20)\n",
    "# parser.add_argument('--pct_traj', type=float, default=1.)\n",
    "# parser.add_argument('--batch_size', type=int, default=64)\n",
    "# parser.add_argument('--model_type', type=str, default='dt')  # dt for decision transformer, bc for behavior cloning\n",
    "# parser.add_argument('--embed_dim', type=int, default=128)\n",
    "# parser.add_argument('--n_layer', type=int, default=3)\n",
    "# parser.add_argument('--n_head', type=int, default=1)\n",
    "# parser.add_argument('--activation_function', type=str, default='relu')\n",
    "# parser.add_argument('--dropout', type=float, default=0.1)\n",
    "# parser.add_argument('--learning_rate', '-lr', type=float, default=1e-4)\n",
    "# parser.add_argument('--weight_decay', '-wd', type=float, default=1e-4)\n",
    "# parser.add_argument('--warmup_steps', type=int, default=10000)\n",
    "# parser.add_argument('--num_eval_episodes', type=int, default=100)\n",
    "# parser.add_argument('--max_iters', type=int, default=10)\n",
    "# parser.add_argument('--num_steps_per_iter', type=int, default=10000)\n",
    "# parser.add_argument('--device', type=str, default='cuda')\n",
    "# parser.add_argument('--log_to_wandb', '-w', type=bool, default=False)\n",
    "\n",
    "\n",
    "class Defaults:\n",
    "    lr: float = 1e-4\n",
    "    weight_decay: float = 1e-4\n",
    "    batch_size: int = 64\n",
    "    warmup_steps: int = 10000\n"
   ],
   "id": "200ba0ac892cf483",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T02:32:25.135264Z",
     "start_time": "2024-05-08T02:32:25.103694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# def eval_episodes(target_rew):\n",
    "#     def fn(model):\n",
    "#         returns, lengths = [], []\n",
    "#         for _ in range(num_eval_episodes):\n",
    "#             with torch.no_grad():\n",
    "#                 if model_type == 'dt':\n",
    "#                     ret, length = evaluate_episode_rtg(\n",
    "#                         env,\n",
    "#                         state_dim,\n",
    "#                         act_dim,\n",
    "#                         model,\n",
    "#                         max_ep_len=max_ep_len,\n",
    "#                         scale=scale,\n",
    "#                         target_return=target_rew/scale,\n",
    "#                         mode=mode,\n",
    "#                         state_mean=state_mean,\n",
    "#                         state_std=state_std,\n",
    "#                         device=device,\n",
    "#                     )\n",
    "#                 else:\n",
    "#                     ret, length = evaluate_episode(\n",
    "#                         env,\n",
    "#                         state_dim,\n",
    "#                         act_dim,\n",
    "#                         model,\n",
    "#                         max_ep_len=max_ep_len,\n",
    "#                         target_return=target_rew/scale,\n",
    "#                         mode=mode,\n",
    "#                         state_mean=state_mean,\n",
    "#                         state_std=state_std,\n",
    "#                         device=device,\n",
    "#                     )\n",
    "#             returns.append(ret)\n",
    "#             lengths.append(length)\n",
    "#         return {\n",
    "#             f'target_{target_rew}_return_mean': np.mean(returns),\n",
    "#             f'target_{target_rew}_return_std': np.std(returns),\n",
    "#             f'target_{target_rew}_length_mean': np.mean(lengths),\n",
    "#             f'target_{target_rew}_length_std': np.std(lengths),\n",
    "#         }\n",
    "#     return fn\n"
   ],
   "id": "ecc75521cf97e1a6",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T02:34:22.605840Z",
     "start_time": "2024-05-08T02:34:21.603630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env = RandomWalkEnv()\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=Defaults.lr,\n",
    "    weight_decay=Defaults.weight_decay,\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer,\n",
    "    lambda steps: min((steps+1)/Defaults.warmup_steps, 1)\n",
    ")\n",
    "\n",
    "env_targets = [0.8] # todo change later\n",
    "\n",
    "trainer = DecisionTransformerTrainer(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        batch_size=Defaults.batch_size,\n",
    "        scheduler=scheduler,\n",
    "        loss_fn=lambda s_hat, a_hat, r_hat, s, a, r: torch.mean((a_hat - a)**2),\n",
    "        device=device,\n",
    "        trajectories=dataset,\n",
    "        eval_fns=[lambda: print(\"make an eval function or I will be called ovvvver and ovvvver again\"),\n",
    "        ]\n",
    "        # eval_fns=[eval_episodes(tar) for tar in env_targets],\n",
    ")\n",
    "\n",
    "\n",
    "trainer.evaluate_episode(env, max_ep_len=10, target_return=1)\n",
    "\n",
    "# dataloader = DataLoader(dataset, batch_size=3, shuffle=True)\n",
    "# observations, actions, rewards, dones, returns_to_go = next(iter(dataloader))\n",
    "# observations, actions, rewards, dones, returns_to_go"
   ],
   "id": "5ce46f93397f7eae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.21753512 -0.54642355 -0.35727021 -0.4843204  -0.19530124 -0.1675347 ]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DecisionTransformer.get_action() missing 2 required positional arguments: 'returns_to_go' and 'timesteps'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[30], line 30\u001B[0m\n\u001B[1;32m     14\u001B[0m env_targets \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m0.8\u001B[39m] \u001B[38;5;66;03m# todo change later\u001B[39;00m\n\u001B[1;32m     16\u001B[0m trainer \u001B[38;5;241m=\u001B[39m DecisionTransformerTrainer(\n\u001B[1;32m     17\u001B[0m         model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[1;32m     18\u001B[0m         optimizer\u001B[38;5;241m=\u001B[39moptimizer,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     26\u001B[0m         \u001B[38;5;66;03m# eval_fns=[eval_episodes(tar) for tar in env_targets],\u001B[39;00m\n\u001B[1;32m     27\u001B[0m )\n\u001B[0;32m---> 30\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate_episode\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_ep_len\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_return\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/projects/RL-credit-assignment-experiment/algorithms/sequence_models/decision_transformer/trainer.py:112\u001B[0m, in \u001B[0;36mDecisionTransformerTrainer.evaluate_episode\u001B[0;34m(self, env, max_ep_len, target_return)\u001B[0m\n\u001B[1;32m    109\u001B[0m actions \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([actions, torch\u001B[38;5;241m.\u001B[39mzeros((\u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mact_dim), device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m    110\u001B[0m rewards \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([rewards, torch\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;241m1\u001B[39m, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)])\n\u001B[0;32m--> 112\u001B[0m action \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_action\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    113\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstates\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    114\u001B[0m \u001B[43m    \u001B[49m\u001B[43mactions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    115\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrewards\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    116\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtarget_return\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtarget_return\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    117\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    118\u001B[0m actions[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m=\u001B[39m action\n\u001B[1;32m    120\u001B[0m action \u001B[38;5;241m=\u001B[39m action\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()\n",
      "\u001B[0;31mTypeError\u001B[0m: DecisionTransformer.get_action() missing 2 required positional arguments: 'returns_to_go' and 'timesteps'"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T03:11:09.103346Z",
     "start_time": "2024-05-08T03:11:08.819618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_ep_len = 100\n",
    "target_return = 1\n",
    "\n",
    "\n",
    "model.eval()\n",
    "model.to(device=device)\n",
    "\n",
    "state, _ = env.reset()\n",
    "state = dataset.state_convertor.to_feature_space(state) # todo changed to trainer\n",
    "print(state)\n",
    "# we keep all the histories on the device\n",
    "# note that the latest action and reward will be \"padding\"\n",
    "states = torch.from_numpy(state).reshape(1, model.state_dim).to(device=device, dtype=torch.float32)\n",
    "actions = torch.zeros((0, model.act_dim), device=device, dtype=torch.float32)\n",
    "rewards = torch.zeros(0, device=device, dtype=torch.float32)\n",
    "target_return = torch.tensor(target_return, device=device, dtype=torch.float32)\n",
    "time_steps = torch.tensor([0], device=device)\n",
    "\n",
    "episode_return, episode_length = 0, 0\n",
    "# for t in range(max_ep_len):\n",
    "\n",
    "    # add padding\n",
    "actions = torch.cat([actions, torch.zeros((1, model.act_dim), device=device)], dim=0)\n",
    "rewards = torch.cat([rewards, torch.zeros(1, device=device)])\n",
    "\n",
    "\n",
    "for i in range(max_ep_len):\n",
    "    # todo paused here\n",
    "    # states, actions, rewards, returns_to_go, timesteps\n",
    "    action = model.get_action(\n",
    "        states=states.to(dtype=torch.float32),\n",
    "        actions=actions.to(dtype=torch.float32),\n",
    "        rewards=rewards.to(dtype=torch.float32),\n",
    "        returns_to_go=target_return,\n",
    "        timesteps=time_steps, # todo change this part\n",
    "    )\n",
    "    actions[-1] = action\n",
    "    time_steps = torch.cat([time_steps, torch.tensor([i], device=device, dtype=torch.long)], dim=0)"
   ],
   "id": "efe93617fd447704",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.21753512 -0.54642355 -0.35727021 -0.4843204  -0.19530124 -0.1675347 ]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[53], line 30\u001B[0m\n\u001B[1;32m     24\u001B[0m rewards \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([rewards, torch\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;241m1\u001B[39m, device\u001B[38;5;241m=\u001B[39mdevice)])\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(max_ep_len):\n\u001B[1;32m     28\u001B[0m     \u001B[38;5;66;03m# todo paused here\u001B[39;00m\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;66;03m# states, actions, rewards, returns_to_go, timesteps\u001B[39;00m\n\u001B[0;32m---> 30\u001B[0m     action \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_action\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     31\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstates\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstates\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     32\u001B[0m \u001B[43m        \u001B[49m\u001B[43mactions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mactions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     33\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrewards\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrewards\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     34\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturns_to_go\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtarget_return\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     35\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtime_steps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;66;43;03m# todo change this part\u001B[39;49;00m\n\u001B[1;32m     36\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     37\u001B[0m     actions[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m=\u001B[39m action\n\u001B[1;32m     38\u001B[0m     time_steps \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([time_steps, torch\u001B[38;5;241m.\u001B[39mtensor([i], device\u001B[38;5;241m=\u001B[39mdevice, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong)], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m~/projects/RL-credit-assignment-experiment/algorithms/sequence_models/decision_transformer/decision_transformer.py:150\u001B[0m, in \u001B[0;36mDecisionTransformer.get_action\u001B[0;34m(self, states, actions, rewards, returns_to_go, timesteps, **kwargs)\u001B[0m\n\u001B[1;32m    147\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    148\u001B[0m     attention_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 150\u001B[0m _, action_preds, return_preds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    151\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstates\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mactions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturns_to_go\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimesteps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    153\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m action_preds[\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n",
      "File \u001B[0;32m~/projects/RL-credit-assignment-experiment/algorithms/sequence_models/decision_transformer/decision_transformer.py:95\u001B[0m, in \u001B[0;36mDecisionTransformer.forward\u001B[0;34m(self, states, actions, rewards, returns_to_go, timesteps, attention_mask)\u001B[0m\n\u001B[1;32m     90\u001B[0m stacked_attention_mask \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mstack(\n\u001B[1;32m     91\u001B[0m     (attention_mask, attention_mask, attention_mask), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     92\u001B[0m )\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mreshape(batch_size, \u001B[38;5;241m3\u001B[39m \u001B[38;5;241m*\u001B[39m seq_length)\n\u001B[1;32m     94\u001B[0m \u001B[38;5;66;03m# we feed in the input embeddings (not word indices as in NLP) to the model\u001B[39;00m\n\u001B[0;32m---> 95\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransformer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstacked_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstacked_attention_mask\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     96\u001B[0m transformer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransformer(\n\u001B[1;32m     97\u001B[0m     inputs_embeds\u001B[38;5;241m=\u001B[39mstacked_inputs,\n\u001B[1;32m     98\u001B[0m     attention_mask\u001B[38;5;241m=\u001B[39mstacked_attention_mask,\n\u001B[1;32m     99\u001B[0m )\n\u001B[1;32m    100\u001B[0m x \u001B[38;5;241m=\u001B[39m transformer_outputs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlast_hidden_state\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "File \u001B[0;32m~/miniconda3/envs/rl-explore/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/rl-explore/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/projects/minGPT/mingpt/model.py:262\u001B[0m, in \u001B[0;36mGPT.forward\u001B[0;34m(self, idx, targets)\u001B[0m\n\u001B[1;32m    260\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, idx, targets\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    261\u001B[0m     device \u001B[38;5;241m=\u001B[39m idx\u001B[38;5;241m.\u001B[39mdevice\n\u001B[0;32m--> 262\u001B[0m     b, t \u001B[38;5;241m=\u001B[39m idx\u001B[38;5;241m.\u001B[39msize()\n\u001B[1;32m    263\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m t \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblock_size, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot forward sequence of length \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mt\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, block size is only \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblock_size\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    264\u001B[0m     pos \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39marange(\u001B[38;5;241m0\u001B[39m, t, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong, device\u001B[38;5;241m=\u001B[39mdevice)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m) \u001B[38;5;66;03m# shape (1, t)\u001B[39;00m\n",
      "\u001B[0;31mValueError\u001B[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T03:10:22.312545Z",
     "start_time": "2024-05-08T03:10:22.275278Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f76007d23de98311",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(1, 48)\n",
       "    (wpe): Embedding(64, 48)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-2): 3 x Block(\n",
       "        (ln_1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=48, out_features=144, bias=True)\n",
       "          (c_proj): Linear(in_features=48, out_features=48, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=48, out_features=192, bias=True)\n",
       "          (c_proj): Linear(in_features=192, out_features=48, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=48, out_features=1, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T02:56:50.886299Z",
     "start_time": "2024-05-08T02:56:50.850081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from mingpt.trainer import Trainer\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "model = DQN(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn()\n",
    "\n",
    "Trainer"
   ],
   "id": "68a9ade1e2161778",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mingpt.trainer.Trainer"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T13:46:59.546259Z",
     "start_time": "2024-05-03T13:46:59.544294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n",
    "# todo probably there is a better way to embed the return. We can try writing it in Gaussian Basis\n",
    "# in the current implementation they just use a linear layer\n"
   ],
   "id": "a05d890d7a1c482f",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T01:00:24.089698Z",
     "start_time": "2024-05-04T01:00:24.029592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, batch_size, dataset, loss_fn, scheduler=None, eval_fns=None):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.train_loader = DataLoader(\n",
    "            dataset,\n",
    "            sampler=torch.utils.data.RandomSampler(dataset, replacement=True, num_samples=int(1e10)),\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=0, # todo or 1?\n",
    "        )\n",
    "\n",
    "        self.loss_fn = loss_fn\n",
    "        self.scheduler = scheduler\n",
    "        self.eval_fns = [] if eval_fns is None else eval_fns\n",
    "        self.diagnostics = dict()\n",
    "\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def train_iteration(self, num_steps, iter_num=0, print_logs=False):\n",
    "\n",
    "        train_losses = []\n",
    "        logs = dict()\n",
    "\n",
    "        train_start = time.time()\n",
    "\n",
    "        self.model.train()\n",
    "        # todo maybe we should pass attention mask from the dataset\n",
    "        # todo for the ones after done, should it only attend to itself or to all the previous ones?\n",
    "        for _, batch in zip(range(num_steps), self.train_loader):\n",
    "            train_loss = self.train_step(batch)\n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            if self.scheduler is not None:\n",
    "                self.scheduler.step()\n",
    "\n",
    "        logs['time/training'] = time.time() - train_start\n",
    "\n",
    "        eval_start = time.time()\n",
    "\n",
    "        self.model.eval()\n",
    "        for eval_fn in self.eval_fns:\n",
    "            outputs = eval_fn(self.model)\n",
    "            for k, v in outputs.items():\n",
    "                logs[f'evaluation/{k}'] = v\n",
    "\n",
    "        logs['time/total'] = time.time() - self.start_time\n",
    "        logs['time/evaluation'] = time.time() - eval_start\n",
    "        logs['training/train_loss_mean'] = np.mean(train_losses)\n",
    "        logs['training/train_loss_std'] = np.std(train_losses)\n",
    "\n",
    "        for k in self.diagnostics:\n",
    "            logs[k] = self.diagnostics[k]\n",
    "\n",
    "        if print_logs:\n",
    "            print('=' * 80)\n",
    "            print(f'Iteration {iter_num}')\n",
    "            for k, v in logs.items():\n",
    "                print(f'{k}: {v}')\n",
    "\n",
    "        return logs\n",
    "\n",
    "    def process_batch(self, batch):\n",
    "        observations, actions, rewards, dones, returns_to_go = batch\n",
    "        \n",
    "        \n",
    "        observations = torch.from_numpy(np.concatenate(observations, axis=0)).to(dtype=torch.float32, device=device)\n",
    "        actions = torch.from_numpy(np.concatenate(actions, axis=0)).to(dtype=torch.float32, device=device)\n",
    "        rewards = torch.from_numpy(np.concatenate(rewards, axis=0)).to(dtype=torch.float32, device=device)\n",
    "        dones = torch.from_numpy(np.concatenate(dones, axis=0)).to(dtype=torch.long, device=device)\n",
    "        rtg = torch.from_numpy(np.concatenate(rtg, axis=0)).to(dtype=torch.float32, device=device)\n",
    "        timesteps = torch.from_numpy(np.concatenate(timesteps, axis=0)).to(dtype=torch.long, device=device)\n",
    "        attention_mask = torch.from_numpy(np.concatenate(attention_mask, axis=0)).to(device=device)\n",
    "\n",
    "        return observations, actions, rewards, dones, attention_mask, returns_to_go\n",
    "    \n",
    "    \n",
    "    def train_step(self, batch):\n",
    "        states, actions, rewards, dones, attention_mask, returns = self.process_batch(batch)\n",
    "        state_target, action_target, reward_target = torch.clone(states), torch.clone(actions), torch.clone(rewards)\n",
    "\n",
    "        state_preds, action_preds, reward_preds = self.model.forward(\n",
    "            states, actions, rewards, masks=None, attention_mask=attention_mask, target_return=returns,\n",
    "        )\n",
    "\n",
    "        # todo wtf! wdym by not fully correct :))\n",
    "        # note: currently indexing & masking is not fully correct\n",
    "        loss = self.loss_fn(\n",
    "            state_preds, action_preds, reward_preds,\n",
    "            state_target[:,1:], action_target, reward_target[:,1:],\n",
    "        )\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.detach().cpu().item()\n",
    "\n",
    "# observations, actions, rewards, dones, returns_to_go"
   ],
   "id": "11f0cafb5244adf9",
   "outputs": [],
   "execution_count": 37
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
