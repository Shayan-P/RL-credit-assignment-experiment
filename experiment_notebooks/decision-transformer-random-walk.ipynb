{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-08T08:16:23.705944Z",
     "start_time": "2024-05-08T08:16:23.646202Z"
    }
   },
   "source": [
    "from torch.utils.data import DataLoader\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "import gymnasium as gym\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "\n",
    "from envs.random_walk import RandomWalkEnv\n",
    "# from algorithms.sequence_models.old_decision_transformer.decision_transformer.decision_transformer import DecisionTransformer\n",
    "from algorithms.sequence_models.decision_transformer.decision_transformer import DecisionTransformer\n",
    "from algorithms.sequence_models.decision_transformer.evaluate import evaluate_on_env\n",
    "from algorithms.sequence_models.old_decision_transformer.decision_transformer.trainer import DecisionTransformerTrainer\n",
    "from data.random_walk_dataset import RandomWalkDataset\n",
    "from data.trajectory import LimitedContextWrapper\n",
    "from settings import LOG_DIR\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T08:16:27.104070Z",
     "start_time": "2024-05-08T08:16:24.051530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rtg_target = 8\n",
    "\n",
    "max_eval_ep_len = 20      # max len of one evaluation episode\n",
    "num_eval_ep = 10          # num of evaluation episodes per iteration\n",
    "\n",
    "batch_size = 64             # training batch size\n",
    "lr = 1e-3                   # learning rate\n",
    "wt_decay = 1e-4             # weight decay\n",
    "warmup_steps = 10000        # warmup steps for lr scheduler\n",
    "\n",
    "# total updates = max_train_iters x num_updates_per_iter\n",
    "max_train_iters = 200\n",
    "num_updates_per_iter = 100\n",
    "\n",
    "context_len = 20        # K in decision transformer\n",
    "n_blocks = 3            # num of transformer blocks\n",
    "embed_dim = 128         # embedding (hidden) dim of transformer\n",
    "n_heads = 1             # num of transformer heads\n",
    "dropout_p = 0.1         # dropout probability\n",
    "\n",
    "env = RandomWalkEnv()\n",
    "traj_dataset = RandomWalkDataset(n_trajectories=10000)"
   ],
   "id": "ae075e310b94044a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Info:\n",
      "episode_max_length: 8\n",
      "reward_scale: 9\n",
      "return min=-17, max=9 mean=-3.4709897147819477\n",
      "state_mean: [0.39712068 0.23109175 0.11509132 0.19179625 0.0370295  0.0278705 ]\n",
      "state_std: [0.48930139 0.42153097 0.31913212 0.39371366 0.18883409 0.16460174]\n",
      "gamma: 1\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T08:16:45.008089Z",
     "start_time": "2024-05-08T08:16:44.946024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Logger:\n",
    "\tdef __init__(self, name):\n",
    "\t\tself.name = name\n",
    "\t\tself.start_time = datetime.datetime.now().replace(microsecond=0)\n",
    "\t\tself.best_score = -np.inf\n",
    "\t\tself.iters = 0\n",
    "\t\tself.num_updates_per_iter = 0\n",
    "\t\tself.previous_csv_extra_keys = None\n",
    "\t\t\n",
    "\t\tself.log_dir = os.path.join(LOG_DIR, \"dt_runs\")\n",
    "\t\t\n",
    "\t\tself.save_model_path = \"\"\n",
    "\t\tself.save_best_model_path = \"\"\n",
    "\n",
    "\t\tif not os.path.exists(self.log_dir):\n",
    "\t\t\tos.makedirs(self.log_dir)\n",
    "\t\t\n",
    "\t\tself.csv_writer = None\n",
    "\t\tself.pbar = None\t\t\n",
    "\t\tself.is_started = False\n",
    "\n",
    "\tdef start(self, iterations, update_per_iter):\t\t\n",
    "\t\tself.start_time = datetime.datetime.now().replace(microsecond=0)\n",
    "\t\tself.best_score = -np.inf\n",
    "\t\tself.iters = 0\n",
    "\t\tself.num_updates_per_iter = update_per_iter\n",
    "\t\tself.previous_csv_extra_keys = None\n",
    "\t\t\n",
    "\t\tprefix = \"dt_\"\n",
    "\t\tstart_time_str = self.start_time.strftime(\"%y-%m-%d-%H-%M-%S\")\n",
    "\t\tsave_model_name = prefix + self.name + \"_model_\" + start_time_str + \".pt\"\n",
    "\t\tself.save_model_path = os.path.join(self.log_dir, save_model_name)\n",
    "\t\tself.save_best_model_path = self.save_model_path[:-3] + \"_best.pt\"\n",
    "\t\tlog_csv_name = prefix + \"_log_\" + start_time_str + \".csv\"\n",
    "\t\tlog_csv_path = os.path.join(self.log_dir, log_csv_name)\n",
    "\t\tself.csv_writer = csv.writer(open(log_csv_path, 'a', 1))\n",
    "\t\t\n",
    "\t\tself.pbar = None\t\t\n",
    "\t\tself.is_started = True\n",
    "\t\tself.pbar = tqdm(total=iterations)\n",
    "\n",
    "\tdef finish(self):\n",
    "\t\tprint(\"=\" * 60)\n",
    "\t\tprint(\"finished training!\")\n",
    "\t\tprint(\"=\" * 60)\n",
    "\t\tend_time = datetime.datetime.now().replace(microsecond=0)\n",
    "\t\ttime_elapsed = str(end_time - self.start_time)\n",
    "\t\tend_time_str = end_time.strftime(\"%y-%m-%d-%H-%M-%S\")\n",
    "\t\t\n",
    "\t\tstart_time_str = self.start_time.strftime(\"%y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "\t\tprint(\"started training at: \" + start_time_str)\n",
    "\t\tprint(\"finished training at: \" + end_time_str)\n",
    "\t\tprint(\"total training time: \" + time_elapsed)\n",
    "\t\tprint(\"best score: \" + format(self.best_score, \".5f\"))\n",
    "\t\tprint(\"saved max d4rl score model at: \" + self.save_best_model_path)\n",
    "\t\tprint(\"saved last updated model at: \" + self.save_model_path)\n",
    "\t\tprint(\"=\" * 60)\n",
    "\n",
    "\t\tself.is_started = False\n",
    "\t\tself.csv_writer.close()\n",
    "\n",
    "\t# todo later make it generic so that we can log whatever\n",
    "\tdef log(self, model, mean_action_loss, eval_avg_reward, important=set(), **kwargs):\t\t\n",
    "\t\tif not self.is_started:\n",
    "\t\t\traise Exception(\"call .start() first\")\n",
    "\n",
    "\t\tif self.previous_csv_extra_keys is None:\n",
    "\t\t\tself.previous_csv_extra_keys = list(kwargs.keys())\n",
    "\t\t\tcsv_header = ([\"duration\", \"num_updates\", \"action_loss\", \"eval_avg_reward\", \"best_score\", *kwargs.keys()])\n",
    "\t\t\tself.csv_writer.writerow(csv_header)\n",
    "\t\telif set(self.previous_csv_extra_keys) != set(kwargs.keys()):\n",
    "\t\t\traise Exception(f\"expected {set(self.previous_csv_extra_keys)} keys but passed {set(kwargs.keys())}. Maybe call finish?\")\n",
    "\t\t\n",
    "\t\tself.iters += 1\n",
    "\t\ttime_elapsed = str(datetime.datetime.now().replace(microsecond=0) - self.start_time)\n",
    "\t\ttotal_updates = self.iters * self.num_updates_per_iter\n",
    "\t\t\t\t\n",
    "\t\tlog_str = '\\n'.join([\n",
    "\t\t\t\"=\" * 60,\n",
    "\t\t\t\"time elapsed: \" + time_elapsed,\n",
    "\t\t\t\"num of updates: \" + str(total_updates),\n",
    "\t\t\t\"action loss: \" +  format(mean_action_loss, \".5f\"),\n",
    "\t\t\t\"eval avg reward: \" + format(eval_avg_reward, \".5f\"),\n",
    "\t\t\t\"best score: \" + format(self.best_score, \".5f\"),\n",
    "\t\t\t*[key + \" \" + format(value, \".5f\") for key, value in kwargs.items()]\n",
    "\t\t])\n",
    "\n",
    "\t\tlog_data = [time_elapsed, total_updates, mean_action_loss,\n",
    "\t\t\t\t\teval_avg_reward, self.best_score] + [kwargs[key] for key in self.previous_csv_extra_keys]\n",
    "\t\tself.csv_writer.writerow(log_data)\n",
    "\t\tif eval_avg_reward >= self.best_score:\n",
    "\t\t\tprint('achieved average reward: ', eval_avg_reward)\n",
    "\t\t\tprint(\"saving max score model at: \" + self.save_best_model_path)\n",
    "\n",
    "\t\t\ttorch.save(model.state_dict(), self.save_best_model_path)\n",
    "\t\t\tself.best_score = eval_avg_reward\n",
    "\t\n",
    "\t\tprint(\"saving current model at: \" + self.save_model_path)\n",
    "\t\ttorch.save(model.state_dict(), self.save_model_path)\n",
    "\t\t\n",
    "\n",
    "\t\tself.pbar.set_description(' '.join([\n",
    "\t\t\tf'Loss={mean_action_loss}',\n",
    "\t\t\tf'Best_Score={self.best_score}',\n",
    "\t\t\t*[f'{key}={value:.5f}' for key, value in kwargs.items() if key in important]\n",
    "\t\t]))\n",
    "\t\t\n",
    "\t\tself.pbar.update(1)\n",
    "\t\t\n",
    "\t\tprint(log_str)"
   ],
   "id": "7bae31db13ba8b6f",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-05-08T08:16:45.481505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "state_dim = traj_dataset.state_dim()\n",
    "act_dim = traj_dataset.action_dim()\n",
    "\n",
    "dataset = LimitedContextWrapper(traj_dataset, context_len=context_len)\n",
    "logger = Logger(name='random-walk')\n",
    "\n",
    "model = DecisionTransformer(\n",
    "\t\t\tstate_dim=state_dim,\n",
    "\t\t\tact_dim=act_dim,\n",
    "\t\t\tn_blocks=n_blocks,\n",
    "\t\t\th_dim=embed_dim,\n",
    "\t\t\tcontext_len=context_len,\n",
    "\t\t\tn_heads=n_heads,\n",
    "\t\t\tdrop_p=dropout_p,\n",
    "\t\t).to(device)\n",
    "  \n",
    "optimizer = torch.optim.AdamW(\n",
    "\t\t\t\t\tmodel.parameters(), \n",
    "\t\t\t\t\tlr=lr, \n",
    "\t\t\t\t\tweight_decay=wt_decay\n",
    "\t\t\t\t)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "\t\toptimizer,\n",
    "\t\tlambda steps: min((steps+1)/warmup_steps, 1)\n",
    "\t)\n",
    "\n",
    "print(\"number of parameters\", sum(np.prod(param.shape) for param in model.parameters()))\n",
    "\n",
    "\n",
    "traj_data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "data_iter = iter(traj_data_loader)\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "logger.start(iterations=max_train_iters, update_per_iter=num_updates_per_iter)\n",
    "\n",
    "for i_train_iter in range(max_train_iters):\n",
    "\n",
    "\tlog_action_losses = []\t\n",
    "\tmodel.train()\n",
    " \n",
    "\tfor _ in range(num_updates_per_iter):\n",
    "\t\ttry:\n",
    "\t\t\ttimesteps, states, actions, returns_to_go, traj_mask = next(data_iter)\n",
    "\t\texcept StopIteration:\n",
    "\t\t\tdata_iter = iter(traj_data_loader)\n",
    "\t\t\ttimesteps, states, actions, returns_to_go, traj_mask = next(data_iter)\n",
    "\n",
    "\t\ttimesteps = timesteps.to(device)\t# B x T\n",
    "\t\tstates = states.to(device)\t\t\t# B x T x state_dim\n",
    "\t\tactions = actions.to(device)\t\t# B x T x act_dim\n",
    "\t\treturns_to_go = returns_to_go.to(device).unsqueeze(dim=-1) # B x T x 1\n",
    "\t\ttraj_mask = traj_mask.to(device)\t# B x T\n",
    "\n",
    "\t\taction_target = torch.clone(actions).detach().to(device)\n",
    "\t\n",
    "\t\tstate_preds, action_preds, return_preds = model.forward(\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\ttimesteps=timesteps,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tstates=states,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tactions=actions,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\treturns_to_go=returns_to_go\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t)\n",
    "\t\t# only consider non padded elements\n",
    "\t\taction_preds = action_preds.view(-1, act_dim)[traj_mask.view(-1,) > 0]\n",
    "\t\taction_target = action_target.view(-1, act_dim)[traj_mask.view(-1,) > 0]\n",
    "\n",
    "\t\t# todo maybe MSE is not the best choice for the discrete actions? Do CrossEntropy\n",
    "\t\taction_loss = loss_fn(action_preds, action_target)\n",
    "\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\taction_loss.backward()\n",
    "\t\ttorch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
    "\t\toptimizer.step()\n",
    "\t\tscheduler.step()\n",
    "\n",
    "\t\tlog_action_losses.append(action_loss.detach().cpu().item())\n",
    "\n",
    "\t# todo evaluate on multiple rtg\n",
    "\t# evaluate on env\n",
    "\tresults = evaluate_on_env(model=model, traj_dataset=traj_dataset,\n",
    "\t\t\t\t\t\t\t  device=device,context_len=context_len,\n",
    "\t\t\t\t\t\t\t  env=env, rtg_target=rtg_target,\n",
    "\t\t\t\t\t\t\t  num_eval_ep=num_eval_ep, max_test_ep_len=max_eval_ep_len)\n",
    "\tlogger.log(model=model,\n",
    "\t\t\t   mean_action_loss=np.mean(log_action_losses),\n",
    "\t\t\t   eval_avg_reward=results['eval/avg_reward'],\n",
    "\t\t\t   eval_avg_ep_len=results['eval/avg_ep_len'],\n",
    "\t\t\t   grad_norm=max(torch.norm(param.grad) for param in model.parameters() if param.grad is not None),\n",
    "\t\t\t   lr=optimizer.param_groups[0]['lr'],\n",
    "\t\t\t   important={\"grad_norm\", \"lr\"})\n",
    "\n",
    "logger.finish()"
   ],
   "id": "b4d041583a7eceee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters 1123085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "488d0e7fccc843b68d0b5ff4cf12a95e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "achieved average reward:  -8.0\n",
      "saving max score model at: /home/shayan/projects/RL-credit-assignment-experiment/logs/dt_runs/dt_random-walk_model_24-05-08-04-16-45_best.pt\n",
      "saving current model at: /home/shayan/projects/RL-credit-assignment-experiment/logs/dt_runs/dt_random-walk_model_24-05-08-04-16-45.pt\n",
      "============================================================\n",
      "time elapsed: 0:00:02\n",
      "num of updates: 100\n",
      "action loss: 0.27092\n",
      "eval avg reward: -8.00000\n",
      "best score: -inf\n",
      "eval_avg_ep_len 8.00000\n",
      "grad_norm 0.19581\n",
      "lr 0.00001\n",
      "achieved average reward:  -8.0\n",
      "saving max score model at: /home/shayan/projects/RL-credit-assignment-experiment/logs/dt_runs/dt_random-walk_model_24-05-08-04-16-45_best.pt\n",
      "saving current model at: /home/shayan/projects/RL-credit-assignment-experiment/logs/dt_runs/dt_random-walk_model_24-05-08-04-16-45.pt\n",
      "============================================================\n",
      "time elapsed: 0:00:03\n",
      "num of updates: 200\n",
      "action loss: 0.11979\n",
      "eval avg reward: -8.00000\n",
      "best score: -8.00000\n",
      "eval_avg_ep_len 8.00000\n",
      "grad_norm 0.09987\n",
      "lr 0.00002\n",
      "achieved average reward:  -8.0\n",
      "saving max score model at: /home/shayan/projects/RL-credit-assignment-experiment/logs/dt_runs/dt_random-walk_model_24-05-08-04-16-45_best.pt\n",
      "saving current model at: /home/shayan/projects/RL-credit-assignment-experiment/logs/dt_runs/dt_random-walk_model_24-05-08-04-16-45.pt\n",
      "============================================================\n",
      "time elapsed: 0:00:05\n",
      "num of updates: 300\n",
      "action loss: 0.09536\n",
      "eval avg reward: -8.00000\n",
      "best score: -8.00000\n",
      "eval_avg_ep_len 8.00000\n",
      "grad_norm 0.15605\n",
      "lr 0.00003\n",
      "achieved average reward:  -8.0\n",
      "saving max score model at: /home/shayan/projects/RL-credit-assignment-experiment/logs/dt_runs/dt_random-walk_model_24-05-08-04-16-45_best.pt\n",
      "saving current model at: /home/shayan/projects/RL-credit-assignment-experiment/logs/dt_runs/dt_random-walk_model_24-05-08-04-16-45.pt\n",
      "============================================================\n",
      "time elapsed: 0:00:06\n",
      "num of updates: 400\n",
      "action loss: 0.09199\n",
      "eval avg reward: -8.00000\n",
      "best score: -8.00000\n",
      "eval_avg_ep_len 8.00000\n",
      "grad_norm 0.11491\n",
      "lr 0.00004\n",
      "achieved average reward:  -8.0\n",
      "saving max score model at: /home/shayan/projects/RL-credit-assignment-experiment/logs/dt_runs/dt_random-walk_model_24-05-08-04-16-45_best.pt\n",
      "saving current model at: /home/shayan/projects/RL-credit-assignment-experiment/logs/dt_runs/dt_random-walk_model_24-05-08-04-16-45.pt\n",
      "============================================================\n",
      "time elapsed: 0:00:07\n",
      "num of updates: 500\n",
      "action loss: 0.08987\n",
      "eval avg reward: -8.00000\n",
      "best score: -8.00000\n",
      "eval_avg_ep_len 8.00000\n",
      "grad_norm 0.10300\n",
      "lr 0.00005\n",
      "achieved average reward:  -8.0\n",
      "saving max score model at: /home/shayan/projects/RL-credit-assignment-experiment/logs/dt_runs/dt_random-walk_model_24-05-08-04-16-45_best.pt\n",
      "saving current model at: /home/shayan/projects/RL-credit-assignment-experiment/logs/dt_runs/dt_random-walk_model_24-05-08-04-16-45.pt\n",
      "============================================================\n",
      "time elapsed: 0:00:09\n",
      "num of updates: 600\n",
      "action loss: 0.08745\n",
      "eval avg reward: -8.00000\n",
      "best score: -8.00000\n",
      "eval_avg_ep_len 8.00000\n",
      "grad_norm 0.12616\n",
      "lr 0.00006\n",
      "achieved average reward:  -8.0\n",
      "saving max score model at: /home/shayan/projects/RL-credit-assignment-experiment/logs/dt_runs/dt_random-walk_model_24-05-08-04-16-45_best.pt\n",
      "saving current model at: /home/shayan/projects/RL-credit-assignment-experiment/logs/dt_runs/dt_random-walk_model_24-05-08-04-16-45.pt\n",
      "============================================================\n",
      "time elapsed: 0:00:10\n",
      "num of updates: 700\n",
      "action loss: 0.08506\n",
      "eval avg reward: -8.00000\n",
      "best score: -8.00000\n",
      "eval_avg_ep_len 8.00000\n",
      "grad_norm 0.14008\n",
      "lr 0.00007\n",
      "achieved average reward:  -8.0\n",
      "saving max score model at: /home/shayan/projects/RL-credit-assignment-experiment/logs/dt_runs/dt_random-walk_model_24-05-08-04-16-45_best.pt\n",
      "saving current model at: /home/shayan/projects/RL-credit-assignment-experiment/logs/dt_runs/dt_random-walk_model_24-05-08-04-16-45.pt\n",
      "============================================================\n",
      "time elapsed: 0:00:12\n",
      "num of updates: 800\n",
      "action loss: 0.08073\n",
      "eval avg reward: -8.00000\n",
      "best score: -8.00000\n",
      "eval_avg_ep_len 8.00000\n",
      "grad_norm 0.11001\n",
      "lr 0.00008\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "92e289508e2aea05"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
