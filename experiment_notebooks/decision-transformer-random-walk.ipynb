{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-03T17:10:56.194444Z",
     "start_time": "2024-05-03T17:10:56.160515Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "from envs.random_walk import RandomWalkEnv\n",
    "from algorithms.random_policy import RandomPolicy\n",
    "from algorithms.sequence_models.decision_transformer.decision_transformer import DecisionTransformer\n",
    "from algorithms.sequence_models.decision_transformer.trainer import DecisionTransformerTrainer\n",
    "from datasets.random_walk_dataset import RandomWalkDataset"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T17:19:43.641975Z",
     "start_time": "2024-05-03T17:19:43.609456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--env', type=str, default='hopper')\n",
    "# parser.add_argument('--dataset', type=str, default='medium')  # medium, medium-replay, medium-expert, expert\n",
    "# parser.add_argument('--mode', type=str, default='normal')  # normal for standard setting, delayed for sparse\n",
    "# parser.add_argument('--K', type=int, default=20)\n",
    "# parser.add_argument('--pct_traj', type=float, default=1.)\n",
    "# parser.add_argument('--batch_size', type=int, default=64)\n",
    "# parser.add_argument('--model_type', type=str, default='dt')  # dt for decision transformer, bc for behavior cloning\n",
    "# parser.add_argument('--embed_dim', type=int, default=128)\n",
    "# parser.add_argument('--n_layer', type=int, default=3)\n",
    "# parser.add_argument('--n_head', type=int, default=1)\n",
    "# parser.add_argument('--activation_function', type=str, default='relu')\n",
    "# parser.add_argument('--dropout', type=float, default=0.1)\n",
    "# parser.add_argument('--learning_rate', '-lr', type=float, default=1e-4)\n",
    "# parser.add_argument('--weight_decay', '-wd', type=float, default=1e-4)\n",
    "# parser.add_argument('--warmup_steps', type=int, default=10000)\n",
    "# parser.add_argument('--num_eval_episodes', type=int, default=100)\n",
    "# parser.add_argument('--max_iters', type=int, default=10)\n",
    "# parser.add_argument('--num_steps_per_iter', type=int, default=10000)\n",
    "# parser.add_argument('--device', type=str, default='cuda')\n",
    "# parser.add_argument('--log_to_wandb', '-w', type=bool, default=False)\n",
    "\n",
    "\n",
    "class Defaults:\n",
    "    lr: float = 1e-4\n",
    "    weight_decay: float = 1e-4\n",
    "    batch_size: int = 64\n",
    "    warmup_steps: int = 10000\n"
   ],
   "id": "200ba0ac892cf483",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def eval_episodes(target_rew):\n",
    "    def fn(model):\n",
    "        returns, lengths = [], []\n",
    "        for _ in range(num_eval_episodes):\n",
    "            with torch.no_grad():\n",
    "                if model_type == 'dt':\n",
    "                    ret, length = evaluate_episode_rtg(\n",
    "                        env,\n",
    "                        state_dim,\n",
    "                        act_dim,\n",
    "                        model,\n",
    "                        max_ep_len=max_ep_len,\n",
    "                        scale=scale,\n",
    "                        target_return=target_rew/scale,\n",
    "                        mode=mode,\n",
    "                        state_mean=state_mean,\n",
    "                        state_std=state_std,\n",
    "                        device=device,\n",
    "                    )\n",
    "                else:\n",
    "                    ret, length = evaluate_episode(\n",
    "                        env,\n",
    "                        state_dim,\n",
    "                        act_dim,\n",
    "                        model,\n",
    "                        max_ep_len=max_ep_len,\n",
    "                        target_return=target_rew/scale,\n",
    "                        mode=mode,\n",
    "                        state_mean=state_mean,\n",
    "                        state_std=state_std,\n",
    "                        device=device,\n",
    "                    )\n",
    "            returns.append(ret)\n",
    "            lengths.append(length)\n",
    "        return {\n",
    "            f'target_{target_rew}_return_mean': np.mean(returns),\n",
    "            f'target_{target_rew}_return_std': np.std(returns),\n",
    "            f'target_{target_rew}_length_mean': np.mean(lengths),\n",
    "            f'target_{target_rew}_length_std': np.std(lengths),\n",
    "        }\n",
    "    return fn\n"
   ],
   "id": "ecc75521cf97e1a6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T14:39:27.168127Z",
     "start_time": "2024-05-03T14:39:09.670428Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env = RandomWalkEnv()\n",
    "dataset = RandomWalkDataset(n_trajectories=100000)\n",
    "\n",
    "model = DecisionTransformer(\n",
    "        hidden_size=64,\n",
    "        dataset=dataset,\n",
    "        block_size=64,  # todo experiment with block_size. we might need to have block_size >= SPLIT_SEQUENCE_LENGTH but I'm not sure about this\n",
    "        max_length=None,\n",
    "        action_tanh=True,\n",
    "        gpt_config={}\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=Defaults.lr,\n",
    "    weight_decay=Defaults.weight_decay,\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer,\n",
    "    lambda steps: min((steps+1)/Defaults.warmup_steps, 1)\n",
    ")\n",
    "\n",
    "env_targets = [0.8]\n",
    "\n",
    "trainer = DecisionTransformerTrainer(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        batch_size=Defaults.batch_size,\n",
    "        scheduler=scheduler,\n",
    "        loss_fn=lambda s_hat, a_hat, r_hat, s, a, r: torch.mean((a_hat - a)**2),\n",
    "        eval_fns=[eval_episodes(tar) for tar in env_targets],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# dataloader = DataLoader(dataset, batch_size=3, shuffle=True)\n",
    "# observations, actions, rewards, dones, returns_to_go = next(iter(dataloader))\n",
    "# observations, actions, rewards, dones, returns_to_go"
   ],
   "id": "5ce46f93397f7eae",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T13:46:59.546259Z",
     "start_time": "2024-05-03T13:46:59.544294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# todo probably there is a better way to embed the return. We can try writing it in Gaussian Basis\n",
    "# in the current implementation they just use a linear layer\n"
   ],
   "id": "a05d890d7a1c482f",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T13:26:07.256305Z",
     "start_time": "2024-05-03T13:26:07.239641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, batch_size, dataset, loss_fn, scheduler=None, eval_fns=None):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.train_loader = DataLoader(\n",
    "            dataset,\n",
    "            sampler=torch.utils.data.RandomSampler(dataset, replacement=True, num_samples=int(1e10)),\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=0, # todo or 1?\n",
    "        )\n",
    "\n",
    "        self.loss_fn = loss_fn\n",
    "        self.scheduler = scheduler\n",
    "        self.eval_fns = [] if eval_fns is None else eval_fns\n",
    "        self.diagnostics = dict()\n",
    "\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def train_iteration(self, num_steps, iter_num=0, print_logs=False):\n",
    "\n",
    "        train_losses = []\n",
    "        logs = dict()\n",
    "\n",
    "        train_start = time.time()\n",
    "\n",
    "        self.model.train()\n",
    "        # todo maybe we should pass attention mask from the dataset\n",
    "        # todo for the ones after done, should it only attend to itself or to all the previous ones?\n",
    "        for _, batch in zip(range(num_steps), self.train_loader):\n",
    "            train_loss = self.train_step(batch)\n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            if self.scheduler is not None:\n",
    "                self.scheduler.step()\n",
    "\n",
    "        logs['time/training'] = time.time() - train_start\n",
    "\n",
    "        eval_start = time.time()\n",
    "\n",
    "        self.model.eval()\n",
    "        for eval_fn in self.eval_fns:\n",
    "            outputs = eval_fn(self.model)\n",
    "            for k, v in outputs.items():\n",
    "                logs[f'evaluation/{k}'] = v\n",
    "\n",
    "        logs['time/total'] = time.time() - self.start_time\n",
    "        logs['time/evaluation'] = time.time() - eval_start\n",
    "        logs['training/train_loss_mean'] = np.mean(train_losses)\n",
    "        logs['training/train_loss_std'] = np.std(train_losses)\n",
    "\n",
    "        for k in self.diagnostics:\n",
    "            logs[k] = self.diagnostics[k]\n",
    "\n",
    "        if print_logs:\n",
    "            print('=' * 80)\n",
    "            print(f'Iteration {iter_num}')\n",
    "            for k, v in logs.items():\n",
    "                print(f'{k}: {v}')\n",
    "\n",
    "        return logs\n",
    "\n",
    "    def process_batch(self, batch):\n",
    "        observations, actions, rewards, dones, returns_to_go = batch\n",
    "        \n",
    "        \n",
    "        observations = torch.from_numpy(np.concatenate(observations, axis=0)).to(dtype=torch.float32, device=device)\n",
    "        actions = torch.from_numpy(np.concatenate(actions, axis=0)).to(dtype=torch.float32, device=device)\n",
    "        rewards = torch.from_numpy(np.concatenate(rewards, axis=0)).to(dtype=torch.float32, device=device)\n",
    "        dones = torch.from_numpy(np.concatenate(dones, axis=0)).to(dtype=torch.long, device=device)\n",
    "        rtg = torch.from_numpy(np.concatenate(rtg, axis=0)).to(dtype=torch.float32, device=device)\n",
    "        timesteps = torch.from_numpy(np.concatenate(timesteps, axis=0)).to(dtype=torch.long, device=device)\n",
    "        attention_mask = torch.from_numpy(np.concatenate(attention_mask, axis=0)).to(device=device)\n",
    "\n",
    "        return observations, actions, rewards, dones, attention_mask, returns_to_go\n",
    "    \n",
    "    \n",
    "    def train_step(self, batch):\n",
    "        states, actions, rewards, dones, attention_mask, returns = self.process_batch(batch)\n",
    "        state_target, action_target, reward_target = torch.clone(states), torch.clone(actions), torch.clone(rewards)\n",
    "\n",
    "        state_preds, action_preds, reward_preds = self.model.forward(\n",
    "            states, actions, rewards, masks=None, attention_mask=attention_mask, target_return=returns,\n",
    "        )\n",
    "\n",
    "        # todo wtf! wdym by not fully correct :))\n",
    "        # note: currently indexing & masking is not fully correct\n",
    "        loss = self.loss_fn(\n",
    "            state_preds, action_preds, reward_preds,\n",
    "            state_target[:,1:], action_target, reward_target[:,1:],\n",
    "        )\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.detach().cpu().item()\n",
    "\n",
    "# observations, actions, rewards, dones, returns_to_go"
   ],
   "id": "11f0cafb5244adf9",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T13:27:32.650025Z",
     "start_time": "2024-05-03T13:27:32.643139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = DecisionTransformer(state_dim=1,\n",
    "                    act_dim=1,\n",
    "                    hidden_size=16, # todo configure for the problem\n",
    "                    max_length=None,\n",
    "                    max_ep_len=16, # todo configure for the problem\n",
    "                    action_tanh=True,\n",
    "                    gpt_config={})"
   ],
   "id": "5c04ccb85312b0b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.09M\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T13:33:14.043889Z",
     "start_time": "2024-05-03T13:33:14.039287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from mingpt.trainer import Trainer\n",
    "\n",
    "\n"
   ],
   "id": "7bc2c105194a5f8c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
